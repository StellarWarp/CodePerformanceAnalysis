import asyncio
import json
import logging
import os
import sys
import traceback
from pathlib import Path
from typing import Annotated, List, Dict, Any, TypedDict, Optional, Literal, AsyncIterator
from dataclasses import dataclass, field

from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END, add_messages
from langgraph.types import Command, interrupt, Interrupt

json_converter_prompt = """# è§’è‰² (Role)
ä½ æ˜¯ä¸€ä¸ªé«˜åº¦ç²¾ç¡®çš„ å·¥ä½œæµåºåˆ—åŒ–å¼•æ“ (Workflow Serialization Engine)ã€‚ä½ çš„ä»»åŠ¡ä¸æ˜¯è®¾è®¡æˆ–ä¿®æ”¹å·¥ä½œæµï¼Œè€Œæ˜¯å°†ä¸€ä¸ªä»¥è‡ªç„¶è¯­è¨€æè¿°çš„ã€å·²ç»ç¡®å®šçš„å·¥ä½œæµæ–¹æ¡ˆï¼Œè½¬æ¢æˆä¸€ä¸ªç»“æ„ä¸¥è°¨ã€å¯ä¾›æœºå™¨è¯»å–çš„JSONæ ¼å¼ã€‚ä½ å¿…é¡»æ³¨é‡ç»†èŠ‚ï¼Œç¡®ä¿100%çš„å‡†ç¡®æ€§ã€‚

# æ ¸å¿ƒç›®æ ‡ (Primary Objective)
ä½ çš„å”¯ä¸€åŠŸèƒ½æ˜¯ï¼šè¯»å–æ‰€æä¾›çš„ã€ç”¨æˆ·ä¸ä»»åŠ¡è§„åˆ’Agentä¹‹é—´çš„å®Œæ•´å¯¹è¯å†å²ï¼Œä»ä¸­è¯†åˆ«å‡ºæœ€ç»ˆè¢«åŒæ–¹ç¡®è®¤é‡‡çº³çš„å·¥ä½œæµæ–¹æ¡ˆï¼Œç„¶åå°†è¯¥æ–¹æ¡ˆçš„æ¯ä¸€ä¸ªæ­¥éª¤éƒ½ç²¾ç¡®åœ°è½¬æ¢ä¸ºJSONå¯¹è±¡æ ¼å¼ï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä¸ªå®Œæ•´çš„JSONã€‚

ä½ å¿…é¡»å¿½ç•¥æ‰€æœ‰åœ¨è®¨è®ºè¿‡ç¨‹ä¸­äº§ç”Ÿçš„è‰ç¨¿ã€è¢«å¦å†³çš„æ–¹æ¡ˆæˆ–ä¸­é—´ç‰ˆæœ¬ã€‚

# è¾“å…¥ (Input)
å¯¹è¯å†å² ({conversation_history}): ä¸€æ®µå®Œæ•´çš„å¯¹è¯æ–‡æœ¬ï¼Œè®°å½•äº†ä»»åŠ¡è§„åˆ’Agentå¦‚ä½•è®¾è®¡å·¥ä½œæµï¼Œä»¥åŠç”¨æˆ·å¦‚ä½•åé¦ˆå¹¶æœ€ç»ˆç¡®è®¤æ–¹æ¡ˆçš„å…¨è¿‡ç¨‹ã€‚

# æ ¸å¿ƒæ‰§è¡Œæµç¨‹ (Core Execution Process)
æ‰«æå¯¹è¯: é€šè¯»æ•´ä¸ªå¯¹è¯å†å²ï¼Œç†è§£å·¥ä½œæµæ–¹æ¡ˆæ˜¯å¦‚ä½•ä»åˆç¨¿æ¼”å˜ä¸ºæœ€ç»ˆç‰ˆæœ¬çš„ã€‚

å®šä½æœ€ç»ˆæ–¹æ¡ˆ: å‡†ç¡®åœ°æ‰¾å‡ºæœ€åè¢«ç”¨æˆ·æ˜ç¡®æˆ–é»˜è®¸é‡‡çº³çš„é‚£ä¸ªå®Œæ•´å·¥ä½œæµç‰ˆæœ¬ã€‚é€šå¸¸ï¼Œè¿™ä¼šæ˜¯è§„åˆ’Agentå‘å‡ºçš„æœ€åä¸€æ¡åŒ…å«å®Œæ•´æ­¥éª¤åˆ—è¡¨çš„æ¶ˆæ¯ï¼Œå¹¶ä¸”ç´§éšå…¶åæœ‰ç”¨æˆ·çš„æ­£é¢ç¡®è®¤ï¼ˆå¦‚ï¼šâ€œå¥½çš„ï¼Œå°±è¿™ä¹ˆåŠâ€ã€â€œå¯ä»¥ï¼Œå¯åŠ¨å§â€ã€â€œæ²¡é—®é¢˜â€ç­‰ï¼‰ã€‚

é€é¡¹æå–: é”å®šæœ€ç»ˆæ–¹æ¡ˆåï¼ŒæŒ‰é¡ºåºéå†å…¶ä¸­çš„æ¯ä¸€ä¸ªæ­¥éª¤ã€‚

ç²¾ç¡®æ˜ å°„: å¯¹äºæ¯ä¸€ä¸ªæ­¥éª¤ï¼Œä»å…¶è‡ªç„¶è¯­è¨€æè¿°ä¸­æå–å…­ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼ˆæ­¥éª¤æ ‡é¢˜ã€ä»»åŠ¡æè¿°ã€å‰ç½®ä¾èµ–ã€è¾“å…¥è§„èŒƒã€è¾“å‡ºè§„èŒƒã€æ‰€éœ€å·¥å…·ï¼‰ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯ä¸¥æ ¼æ˜ å°„åˆ°ä¸‹æ–¹ # è¾“å‡ºJSONç»“æ„å®šä¹‰ ä¸­æŒ‡å®šçš„å­—æ®µã€‚

ç”Ÿæˆå¹¶éªŒè¯: å°†æ‰€æœ‰æ­¥éª¤çš„JSONå¯¹è±¡ç»„åˆæˆä¸€ä¸ªåˆ—è¡¨ï¼Œå¹¶å°†å…¶æ”¾å…¥æœ€ç»ˆçš„æ ¹JSONå¯¹è±¡ä¸­ã€‚ç¡®ä¿æœ€ç»ˆè¾“å‡ºçš„æ˜¯ä¸€ä¸ªå•ä¸€ã€å®Œæ•´ä¸”è¯­æ³•æ­£ç¡®çš„JSONæ–‡æœ¬ã€‚ä¸è¦åœ¨JSONä»£ç å—å‰åæ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ã€æ³¨é‡Šæˆ–Markdownæ ‡è®°ã€‚
"""


# --- Configuration ---
@dataclass
class AgentConfig:
    """ä»£ç†é…ç½®ç±»"""
    model_name: str = "deepseek-chat"
    api_key: str = os.environ['DEEPSEEK_API_KEY']  # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–
    base_url: str = "https://api.deepseek.com/v1"
    mcp_servers: Dict[str, Dict[str, str]] = field(default_factory=lambda: {
        "Unreal insight Call Tree": {
            'url': "http://localhost:8001/sse/",
            "transport": "sse"
        },
        "CodeGraph": {
            'url': "http://localhost:8000/mcp/",
            "transport": "streamable_http"
        }
    })
    thread_id: str = "human-in-the-loop-thread"

    # æ–‡æ¡£è·¯å¾„é…ç½®
    experience_doc_path: str = "doc/experience_doc"
    system_prompt_path: str = "doc/system_prompt"
    user_prompt_path: str = "doc/user_prompt"
    utrace_file_path: str = r'C:\Users\lyq\Desktop\Work\CodePerformanceAnalysis\data\CSV\Test\20250626_215834.utrace'


# --- State Definition ---
class PlanningState(TypedDict):
    """ä»£ç†çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[AnyMessage], add_messages]
    plan_approved: bool
    revision_count: int
    current_plan: Optional[str]
    final_json: Optional[str]


# --- Document Manager ---
class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨"""

    def __init__(self, config: AgentConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def read_file_safely(self, file_path: str, default_content: str = "") -> str:
        """å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            self.logger.warning(f"æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹")
            return default_content
        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡ä»¶ '{file_path}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return default_content

    def read_experience_doc(self) -> str:
        """è¯»å–ç»éªŒæ–‡æ¡£"""
        return self.read_file_safely(self.config.experience_doc_path)

    def read_system_prompt(self) -> str:
        """è¯»å–ç³»ç»Ÿæç¤º"""
        system_prompt = self.read_file_safely(
            self.config.system_prompt_path,
            ""
        )
        return system_prompt.replace("{experience_docs}", self.read_experience_doc())

    def read_user_prompt(self) -> str:
        """è¯»å–ç”¨æˆ·æç¤º"""
        user_prompt = self.read_file_safely(
            self.config.user_prompt_path,
            ""
        )
        return user_prompt.format(utrace_file=self.config.utrace_file_path)


# --- Streaming Helper Functions ---
def print_stream_chunk(chunk: str, end: str = ""):
    """æ‰“å°æµå¼è¾“å‡ºå—"""
    print(chunk, end=end, flush=True)


async def stream_llm_response(llm, messages: List[AnyMessage], prefix: str = "") -> str:
    """æµå¼è°ƒç”¨LLMå¹¶è¿”å›å®Œæ•´å“åº”"""
    if prefix:
        print(f"\n{prefix}")

    full_response = ""

    try:
        # ä½¿ç”¨astreamè¿›è¡Œæµå¼è°ƒç”¨
        async for chunk in llm.astream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                content = chunk.content
                print_stream_chunk(content)
                full_response += content
    except Exception as e:
        error_msg = f"æµå¼è°ƒç”¨LLMæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print_stream_chunk(error_msg)
        full_response = error_msg

    print()  # æ¢è¡Œ
    return full_response


# --- Graph Nodes ---
class PlanningNodes:
    """è§„åˆ’èŠ‚ç‚¹ç±»"""

    def __init__(self, llm, config: AgentConfig):
        self.llm = llm
        self.config = config
        self.logger = logging.getLogger(__name__)

    def planner_node(self, state: PlanningState) -> Dict[str, Any]:
        """è§„åˆ’èŠ‚ç‚¹ï¼šç”Ÿæˆæˆ–ä¿®è®¢è®¡åˆ’"""
        revision_count = state.get('revision_count', 0)

        if revision_count == 0:
            prefix = "ğŸ¤– æ­£åœ¨ç”Ÿæˆåˆå§‹è®¡åˆ’..."
        else:
            prefix = f"ğŸ¤– æ­£åœ¨ä¿®è®¢è®¡åˆ’ (ç¬¬ {revision_count} æ¬¡ä¿®è®¢)..."

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºæ–°çš„
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨ run_coroutine_threadsafe
                    import concurrent.futures
                    import threading

                    def run_in_new_loop():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(
                                stream_llm_response(self.llm, state['messages'], prefix)
                            )
                        finally:
                            new_loop.close()

                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_in_new_loop)
                        response_content = future.result()
                else:
                    response_content = loop.run_until_complete(
                        stream_llm_response(self.llm, state['messages'], prefix)
                    )
            except RuntimeError:
                # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                response_content = asyncio.run(
                    stream_llm_response(self.llm, state['messages'], prefix)
                )

            # åˆ›å»ºAIæ¶ˆæ¯
            response = AIMessage(content=response_content)

            # æ›´æ–°çŠ¶æ€
            return {
                "messages": [response],
                "current_plan": response_content,
                "revision_count": revision_count + 1
            }
        except Exception as e:
            self.logger.error(f"è§„åˆ’èŠ‚ç‚¹æ‰§è¡Œé”™è¯¯: {e}")
            error_msg = AIMessage(content=f"ç”Ÿæˆè®¡åˆ’æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {"messages": [error_msg]}

    def human_approval_node(self, state: PlanningState) -> Dict[str, Any]:
        """äººå·¥å®¡æ ¸èŠ‚ç‚¹"""
        current_plan = state.get('current_plan', "")
        revision_count = state.get('revision_count', 0)

        interrupt_data = {
            "question": "è¯·å®¡æ ¸ä»¥ä¸‹è®¡åˆ’ã€‚å¦‚æœæ»¡æ„ï¼Œè¯·è¾“å…¥ 'approved'ï¼›å¦åˆ™è¯·æä¾›å…·ä½“ä¿®æ”¹æ„è§ï¼š",
            "current_plan": current_plan,
            "revision_count": revision_count
        }
        user_feedback = interrupt(interrupt_data)
        return {"messages": [HumanMessage(content=user_feedback)]}


    def json_converter_node(self, state: PlanningState) -> Dict[str, Any]:
        """JSONè½¬æ¢èŠ‚ç‚¹ï¼šå°†æ‰¹å‡†çš„è®¡åˆ’è½¬æ¢ä¸ºJSONæ ¼å¼"""
        json_instruction = HumanMessage(content=json_converter_prompt)

        try:
            # å‡†å¤‡å®Œæ•´çš„æ¶ˆæ¯å†å²
            messages_for_conversion = state['messages'] + [json_instruction]

            # ä½¿ç”¨æµå¼è¾“å‡ºè¿›è¡ŒJSONè½¬æ¢ï¼Œå¤„ç†äº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    import concurrent.futures

                    def run_in_new_loop():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(
                                stream_llm_response(
                                    self.llm,
                                    messages_for_conversion,
                                    "ğŸ”„ æ­£åœ¨å°†è®¡åˆ’è½¬æ¢ä¸ºJSONæ ¼å¼..."
                                )
                            )
                        finally:
                            new_loop.close()

                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_in_new_loop)
                        response_content = future.result()
                else:
                    response_content = loop.run_until_complete(
                        stream_llm_response(
                            self.llm,
                            messages_for_conversion,
                            "ğŸ”„ æ­£åœ¨å°†è®¡åˆ’è½¬æ¢ä¸ºJSONæ ¼å¼..."
                        )
                    )
            except RuntimeError:
                response_content = asyncio.run(
                    stream_llm_response(
                        self.llm,
                        messages_for_conversion,
                        "ğŸ”„ æ­£åœ¨å°†è®¡åˆ’è½¬æ¢ä¸ºJSONæ ¼å¼..."
                    )
                )

            # åˆ›å»ºAIæ¶ˆæ¯
            response = AIMessage(content=response_content)

            return {
                "messages": [response],
                "final_json": response_content,
                "plan_approved": True
            }
        except Exception as e:
            self.logger.error(f"JSONè½¬æ¢èŠ‚ç‚¹æ‰§è¡Œé”™è¯¯: {e}")
            error_msg = AIMessage(content=f"è½¬æ¢ä¸ºJSONæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {"messages": [error_msg]}


# --- Routing Functions ---
def route_after_human_review(state: PlanningState) -> Literal["planner_node", "json_converter_node"]:
    """äººå·¥å®¡æ ¸åçš„è·¯ç”±å‡½æ•°"""
    last_message = state['messages'][-1]

    if isinstance(last_message, HumanMessage):
        feedback = last_message.content.lower().strip()

        if 'approved' in feedback:
            return 'json_converter_node'
        else:
            return 'planner_node'

    # é»˜è®¤è¿”å›è§„åˆ’èŠ‚ç‚¹
    return 'planner_node'


# --- Async Main Agent Class ---
class PlanningAgent:
    """ä¸»ä»£ç†ç±»"""

    def __init__(self, config: AgentConfig):
        self.config = config
        self.doc_manager = DocumentManager(config)
        self.logger = logging.getLogger(__name__)
        self.llm = None
        self.graph = None
        self.memory = MemorySaver()

    async def initialize(self):
        """åˆå§‹åŒ–ä»£ç†"""
        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model=self.config.model_name,
            api_key=self.config.api_key,
            base_url=self.config.base_url,
            streaming=True,  # å¯ç”¨æµå¼è¾“å‡º
        )

        # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯
        try:
            client = MultiServerMCPClient(self.config.mcp_servers)
            tools = await client.get_tools()
            self.llm = self.llm.bind_tools(tools)
            self.logger.info("MCPå·¥å…·ç»‘å®šæˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"MCPå·¥å…·ç»‘å®šå¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨åŸºç¡€LLM")

        # æ„å»ºå›¾
        self._build_graph()

    def _build_graph(self):
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        nodes = PlanningNodes(self.llm, self.config)

        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(PlanningState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("planner_node", nodes.planner_node)
        workflow.add_node("human_approval_node", nodes.human_approval_node)
        workflow.add_node("json_converter_node", nodes.json_converter_node)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("planner_node")

        # æ·»åŠ è¾¹
        workflow.add_edge("planner_node", "human_approval_node")
        workflow.add_edge("json_converter_node", END)

        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "human_approval_node",
            route_after_human_review,
            {
                "planner_node": "planner_node",
                "json_converter_node": "json_converter_node"
            }
        )

        # ç¼–è¯‘å›¾
        self.graph = workflow.compile(checkpointer=self.memory)

    async def run(self) -> Dict[str, Any]:
        """è¿è¡Œä»£ç†"""
        if not self.graph:
            raise RuntimeError("ä»£ç†æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize() æ–¹æ³•")

        # å‡†å¤‡åˆå§‹è¾“å…¥
        system_prompt = self.doc_manager.read_system_prompt()
        user_prompt = self.doc_manager.read_user_prompt()

        config = {"configurable": {"thread_id": self.config.thread_id}}

        initial_input = {
            "messages": [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ],
            "plan_approved": False,
            "revision_count": 0,
            "current_plan": None,
            "final_json": None
        }

        # æ‰§è¡Œå›¾
        try:
            response = self.graph.invoke(initial_input, config)

            # å¤„ç†äººå·¥äº¤äº’å¾ªç¯
            while True:
                current_state = self.graph.get_state(config=config)

                if len(current_state.next) == 0:
                    self.logger.info("ğŸ‰ ä»£ç†æ‰§è¡Œå®Œæˆ")
                    break

                # æ˜¾ç¤ºå½“å‰è®¡åˆ’ä¾›å®¡æ ¸
                if response.get("__interrupt__"):
                    interrupt_data = response["__interrupt__"][-1]
                    self._display_plan_for_review(interrupt_data)

                    # è·å–ç”¨æˆ·è¾“å…¥
                    human_input = input("\nè¯·è¾“å…¥æ‚¨çš„åé¦ˆ: ").strip()

                    # ç»§ç»­æ‰§è¡Œ
                    response = self.graph.invoke(Command(resume=human_input), config=config)

            # è·å–æœ€ç»ˆç»“æœ
            final_state = self.graph.get_state(config)
            return self._extract_final_result(final_state)

        except Exception as e:
            traceback.print_exc()
            self.logger.error(f"ä»£ç†æ‰§è¡Œé”™è¯¯: {e}")
            raise

    def _display_plan_for_review(self, interrupt_data: Interrupt):
        """æ˜¾ç¤ºè®¡åˆ’ä¾›å®¡æ ¸"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ è®¡åˆ’å®¡æ ¸")
        print("=" * 60)

        # ç”±äºç°åœ¨ä½¿ç”¨æµå¼è¾“å‡ºï¼Œå½“å‰è®¡åˆ’å·²ç»åœ¨æµå¼è¾“å‡ºä¸­æ˜¾ç¤ºäº†
        # è¿™é‡Œåªæ˜¾ç¤ºé¢å¤–çš„å®¡æ ¸ä¿¡æ¯

        if "revision_count" in interrupt_data.value:
            print(f"ä¿®è®¢æ¬¡æ•°: {interrupt_data.value['revision_count']}")

        print("\n" + interrupt_data.value.get("question", "è¯·å®¡æ ¸è®¡åˆ’"))
        print("=" * 60)
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'approved' æ‰¹å‡†è®¡åˆ’ï¼Œæˆ–è¾“å…¥å…·ä½“ä¿®æ”¹æ„è§")

    def _extract_final_result(self, final_state) -> Dict[str, Any]:
        """æå–æœ€ç»ˆç»“æœ"""
        state_values = final_state.values

        # è·å–æœ€ç»ˆçš„AIæ¶ˆæ¯
        final_ai_message = None
        for msg in reversed(state_values['messages']):
            if isinstance(msg, AIMessage):
                final_ai_message = msg.content
                break

        result = {
            "final_plan": final_ai_message,
            "revision_count": state_values.get('revision_count', 0),
            "plan_approved": state_values.get('plan_approved', False),
            "final_json": state_values.get('final_json')
        }

        return result


# --- Main Function ---
async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆ›å»ºé…ç½®ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–ï¼‰
    config = AgentConfig(
        api_key="sk-7b32e9e21abd4eea9c7a9db728401324",  # â—ï¸ è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥
        thread_id="planning-agent-thread-v2"
    )

    # åˆ›å»ºå¹¶è¿è¡Œä»£ç†
    agent = PlanningAgent(config)

    try:
        await agent.initialize()
        result = await agent.run()

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ æœ€ç»ˆç»“æœ")
        print("=" * 60)
        print(f"ä¿®è®¢æ¬¡æ•°: {result['revision_count']}")
        print(f"è®¡åˆ’å·²æ‰¹å‡†: {result['plan_approved']}")

        # æœ€ç»ˆè®¡åˆ’å·²ç»é€šè¿‡æµå¼è¾“å‡ºæ˜¾ç¤ºäº†ï¼Œè¿™é‡Œåªæ˜¾ç¤ºæ‘˜è¦
        if result['final_json']:
            print("\nğŸ“„ æœ€ç»ˆJSONæ ¼å¼å·²ç”Ÿæˆå®Œæˆ")
        else:
            print("\nğŸ“‹ æœ€ç»ˆè®¡åˆ’å·²ç”Ÿæˆå®Œæˆ")

        print("=" * 60)

    except Exception as e:
        logging.error(f"ä»£ç†æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == '__main__':

    asyncio.run(main())